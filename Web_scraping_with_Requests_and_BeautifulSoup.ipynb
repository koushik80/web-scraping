{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping with requests and beautifulsoup\n",
    "\n",
    "- https://docs.python-requests.org/en/latest/\n",
    "- https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "Install modules with:\n",
    "```\n",
    "pip install requests\n",
    "pip install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import requests module\n",
    "```Python\n",
    "import requests\n",
    "```\n",
    "\n",
    "## Make a basic request with requests module\n",
    "```Python\n",
    "res = requests.get('https://google.com')\n",
    "```\n",
    "\n",
    "Get method returns a response object. Before doind anything else it's better to check did the request went through:\n",
    "```Python\n",
    "if not res.ok:\n",
    "    print(\"Error: Webpage can't be downloaded\")\n",
    "\n",
    "# or:\n",
    "\n",
    "if res.status_code < 400:\n",
    "    # do something with res\n",
    "    # webpage content is available in res.text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requests functions\n",
    "\n",
    "- `delete(url, args)`: Sends a DELETE request to the specied url.\n",
    "- `get(url, args)`: Sends a GET request to the specied url.\n",
    "- `head(url, args)`: Sends a HEAD request to the specied url.\n",
    "- `patch(url, args)`: Sends a PATCH request to the specied url.\n",
    "- `post(url, args)`: Sends a POST request to the specied url.\n",
    "- `options(url, args)`: Sends an OPTIONS request to the specied url.\n",
    "- `put(url, args)`: Sends a PUT request to the specied url.\n",
    "- `request(method, url, args)`: Sends a requests of the specified [method](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods) to the specified url.\n",
    "\n",
    "The most used and common request type is **get**.\n",
    "\n",
    "### Common args for functions:\n",
    "- `auth`: Auth tuple to enable Basic/Digest/Custom HTTP Auth.\n",
    "- `cookies`: Dict or CookieJar object to send with the Request.\n",
    "- `data`: Dictionary, list of tuples, bytes, or file-like object to send in the body of the Request.\n",
    "- `files`: Dictionary of 'name': file-like-objects (or {'name': file-tuple}) for multipart encoding upload.\n",
    "- `headers`: Dictionary of HTTP Headers to send with the Request.\n",
    "- `json`: A JSON serializable Python object to send in the body of the Request.\n",
    "- `params`: Only for `get`. Dictionary, list of tuples or bytes to send in the query string for the Request.\n",
    "- `timeout`: How many seconds to wait for the server to send data before giving up, as a float, or a (connect timeout, read timeout) tuple.\n",
    "\n",
    "## Other requests module attributes\n",
    "\n",
    "- `codes`: The `codes` object defines a mapping from common names for HTTP statuses to their numerical codes in dict form (requests.structures.LookupDict)\n",
    "- `requests.utils.get_encodings_from_content(content)`: Returns encodings from given content string.\n",
    "- `requests.utils.get_encoding_from_headers(headers)`: Returns encodings from given HTTP Header Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response object\n",
    "\n",
    "- `apparent_encoding`: The apparent encoding of the response (str)\n",
    "- `close()`: Releases the connection back to the pool.\n",
    "- `connection`: Connection object (requests.adapters.HTTPAdapter)\n",
    "- `content`: Content of the response in bytes (raw content) (bytes)\n",
    "- `cookies`: Cookies object (requests.cookies.RequestsCookieJar)\n",
    "- `elapsed`: Timedelta object with the time elapsed from sending the request to the arrival of the response (datetime.timedelta)\n",
    "- `encoding`: Encoding used to decode to response.text (str)\n",
    "- `headers`: Response headers dict object (requests.structures.CaseInsensitiveDict)\n",
    "- `history`: Response history list (list)\n",
    "- `is_permanent_redirect`: (bool)\n",
    "- `is_redirect`: (bool)\n",
    "- `iter_content()`: Iterates over the response data.\n",
    "- `iter_lines()`: Iterates over the response data, one line at a time.\n",
    "- `json()`: Returns the json-encoded content of a response, if any.\n",
    "- `links`: Response header links (dict)\n",
    "- `next`: PreparedRequest object for the next request in a redirection (ResponseObject)\n",
    "- `ok`: True if status_code is less than 400, otherwise False (bool)\n",
    "- `raise_for_status()`: Raises `HTTPError`, if one occurred.\n",
    "- `raw`: Raw response object (urllib3.response.HTTPResponse)\n",
    "- `reason`: Text corresponding to the status code (str)\n",
    "- `request`: Request object that requested this response (requests.models.PreparedRequest)\n",
    "- `status_code`: Number that indicates the [HTTP status](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) of the response (int)\n",
    "- `text`: The content of the response, in UTF-8 (str)\n",
    "- `url`: The URL of the response (str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the BeautifulSoup module\n",
    "\n",
    "How to import Beautiful Soup:\n",
    "```Python\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "\n",
    "## Starting the BeautifulSoup parser\n",
    "\n",
    "After that, create a soup element to work with:\n",
    "```Python\n",
    "# Start the parser (basic parser: 'html.parser')\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "```\n",
    "\n",
    "## BeautifulSoup object types\n",
    "\n",
    "There are four basic object types:\n",
    "- `BeautifulSoup`: It's the basic element, that contains all the source code\n",
    "- `Tag`: It's an HTML element/Tag node object. It contains an HTML element including it's descendants\n",
    "- `NavigableString`: It's a text node element. Every piece of text it's contained in one object of this type (One object for every peace of text), including possible new lines between some HTML elements.\n",
    "- `Comment`: It's a comment node element.\n",
    "\n",
    "There are more types, but these are the more common ones.\n",
    "\n",
    "## Locate elements\n",
    "\n",
    "Find the first element of a certain tag name: soup.tag_name\n",
    "```Python\n",
    "p = soup.p      # First paragraph\n",
    "h1 = soup.h1    # First header h1\n",
    "img = soup.img  # First image\n",
    "li = soup.ul.li # Firs list item in the first unordered list\n",
    "```\n",
    "\n",
    "p, h1, img and li are Tag objects. More searches can be made on those objects if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag object methods\n",
    "\n",
    "### Altering/Modifying\n",
    "\n",
    "- `append()`: Appends the given PageElement to the contents of this one.\n",
    "- `clear()`: Wipe out all children of this PageElement by calling extract() on them.\n",
    "- `decompose()`: Recursively destroys this PageElement and its children.\n",
    "- `extend()`: Appends the given PageElements to this one's contents.\n",
    "- `extract()`: Destructively rips this element out of the tree.\n",
    "- `insert()`: Insert a new PageElement in the list of this PageElement's children.\n",
    "- `insert_after()`: Makes the given element(s) the immediate successor of this one.\n",
    "- `insert_before()`: Makes the given element(s) the immediate predecessor of this one.\n",
    "- `replace_with()`: Replace this PageElement with one or more PageElements, keeping the rest of the tree the same.\n",
    "- `replace_with_children()`: Replace this PageElement with its contents.\n",
    "- `smooth()`: Smooth out this element's children by consolidating consecutive strings.\n",
    "- `unwrap()`: Replace this PageElement with its contents.\n",
    "- `wrap()`: Wrap this PageElement inside another one.\n",
    "\n",
    "### Finding/Locating\n",
    "\n",
    "- `find()`: Look in the children of this PageElement and find the first PageElement that matches the given criteria.\n",
    "- `find_all()`: Look in the children of this PageElement and find all PageElements that match the given criteria.\n",
    "- `find_all_next()`: Find all PageElements that match the given criteria and appear later in the document than this PageElement.\n",
    "- `find_all_previous()`: Look backwards in the document from this PageElement and find all PageElements that match the given criteria.\n",
    "- `find_next()`: Find the first PageElement that matches the given criteria and appears later in the document than this PageElement.\n",
    "- `find_next_sibling()`: Find the closest sibling to this PageElement that matches the given criteria and appears later in the document.\n",
    "- `find_next_siblings()`: Find all siblings of this PageElement that match the given criteria and appear later in the document.\n",
    "- `find_parent()`: Find the closest parent of this PageElement that matches the given criteria.\n",
    "- `find_parents()`: Find all parents of this PageElement that match the given criteria.\n",
    "- `find_previous()`: Look backwards in the document from this PageElement and find the first PageElement that matches the given criteria.\n",
    "- `find_previous_sibling()`: Returns the closest sibling to this PageElement that matches the given criteria and appears earlier in the document.\n",
    "- `find_previous_siblings()`: Returns all siblings to this PageElement that match the given criteria and appear earlier in the document.\n",
    "- `select()`: Perform a CSS selection operation on the current element.\n",
    "- `select_one()`: Perform a CSS selection operation on the current element.\n",
    "\n",
    "### Others\n",
    "\n",
    "- `decode()`: Render a Unicode representation of this PageElement and its contents.\n",
    "- `decode_contents()`: Renders the contents of this tag as a Unicode string.\n",
    "- `encode()`: Render a bytestring representation of this PageElement and its contents.\n",
    "- `encode_contents()`: Renders the contents of this PageElement as a bytestring.\n",
    "- `get()`: Returns the value of the 'key' attribute for the tag, or the value given for 'default' if it doesn't have that attribute.\n",
    "- `get_attribute_list()`: The same as get(), but always returns a list.\n",
    "- `get_text()`: Get all child strings of this PageElement, concatenated using the given separator.\n",
    "- `has_attr()`: Does this PageElement have an attribute with the given name?.\n",
    "- `index()`: Find the index of a child by identity, not value.\n",
    "- `prettify()`: Pretty-print this PageElement as a string.\n",
    "\n",
    "\n",
    "## Tag object attributes\n",
    "\n",
    "- `attrs`: Dictionary with Tag's HTML attributes (dict)\n",
    "- `children`: Tag's (direct) childrens as an iterator (list_iterator)\n",
    "- `contents`: Tag's (direct) childrens as a list (list)\n",
    "- `decomposed`: Has node been decomposed? (bool)\n",
    "- `descendants`: Tag's descendats as an iterator (generator)\n",
    "- `is_empty_element`: Is this a self-closing element, like img? (bool)\n",
    "- `name`: Tag name (str)\n",
    "- `namespace`: XML namespace (str)\n",
    "- `next_element`: Next node (text, tag, comment, ...) after this tag's start tag (Also inside current tag)(bs4 object)\n",
    "- `next_elements`: Generator for next elements (generator)\n",
    "- `next_sibling`: Next sibling node after this tag (skips this tag's content) (bs4 object)\n",
    "- `next_siblings`: Generator for next siblings (generator)\n",
    "- `parent`: Tag's direct parent tag/element (bs4.element.Tag)\n",
    "- `parents`: Tag's parents (ascendants) as generator (generator)\n",
    "- `previous_element`: Previous node (text, tag, comment, ...) before this tag (bs4 object)\n",
    "- `previous_elements`: Generator for previous elements/tags (generator)\n",
    "- `previous_sibling`: Previous sibling node before this tag (bs4 object)\n",
    "- `previous_siblings`: Generator for previous siblings (generator)\n",
    "- `sourceline`: Position row in source code (int)\n",
    "- `sourcepos`: Position column in source code (int)\n",
    "- `string`: Tag's text node as NavigableString (can also be this tag's only child's string) (bs4.element.NavigableString)\n",
    "- `strings`: Generator for all text contained in the tag (including descendants) (generator)\n",
    "- `stripped_strings`: Same as previous but strings are stripped (generator)\n",
    "- `text`: Tag's contained text (includes text from descendants) (str)\n",
    "\n",
    "Tag's HTML attributes can be accessed as in a dictionary:\n",
    "```Python\n",
    "tag['id']       # Tag's ID attribute\n",
    "tag['class']    # Tag's class attribute\n",
    "tag['name']     # Tag's name attribute\n",
    "tag['style']    # Tag's style attribute\n",
    "\n",
    "# See also tag.attrs\n",
    "```\n",
    "\n",
    "Two main function/methods for locating elements/tags:\n",
    "- `find`: Find the first element/tag to match the search criteria. Returns an object.\n",
    "- `find_all`: Find all the elements/tags that match the search criteria. Returns a list.\n",
    "\n",
    "Anyway the rest of find_* functions work similary.\n",
    "\n",
    "You can use different search criteria to filter based on a tag’s name, on its attributes, on the text of a string, or on some combination of these.\n",
    "- A **string** will find matching tag names. The `name` parameter can also be used.\n",
    "```Python\n",
    "soup.find('b')          # Find first 'b' tag\n",
    "soup.find_all('b')      # Find all 'b' tags (returns a list)\n",
    "soup.find_all(name='b') # Find all 'b' tags (returns a list)\n",
    "```\n",
    "- A **regular expression** (using `re` module) that matches against tag names\n",
    "```Python\n",
    "sound.find_all(re.compile('^b'))     # Find tags like 'b' and 'body' (start with b)\n",
    "```\n",
    "- A **list** will find tag names included in the list\n",
    "```Python\n",
    "soup.find_all(['img', 'a'])     # Find img and a elements\n",
    "```\n",
    "- A **True** value will find all tags\n",
    "- A custom **Function** can be passed that find tags on user's own criteries. Function's parameter is a tag and it should return True if matches and False otherwise.\n",
    "- An **HTML attribute** in the form \"key=value\". Multiple attributes can be defined. As `class` is a Python reserved keyword and can't be used, `class_` should be used instead. Element's **text** can also be searched, using the `string` attribute. A regular expression can also be used as value.\n",
    "```Python\n",
    "soup.find_all(id='main', class_='important') # Find all elements with given attributes\n",
    "soup.find_all(string='Download')             # Find all elements with exact match text\n",
    "soup.find_all(href=re.compile('php'))        # Find links that contains the word 'php'\n",
    "```\n",
    "- Multiple criteria can be combined together:\n",
    "```Python\n",
    "soup.find('a', id='main')   # Find a element with given id.\n",
    "```\n",
    "- Some attributes, like the data-* attributes in HTML 5, have names that can’t be used as the names of keyword arguments. A search can be done on those using the `attrs` parameter:\n",
    "```Python\n",
    "soup.find(attrs={\"data-foo\": \"value\"})  # Find element with data-foo attribute equal to value\n",
    "```\n",
    "- Also, the `name` HTML attribute can't be searched directly, as `name` parameter is reserved for searching tag names. `attrs` parameter should be used instead:\n",
    "```Python\n",
    "soup.find(attrs={\"name\": \"email\"})\n",
    "```\n",
    "- Parameters's value can be a string, regular expression (using re), a boolean value or a function:\n",
    "    - String means exact match against the string. In case of an attribute with multiple possible values (like class), the search will try to match every value separately.\n",
    "    - Regular expressions. Same as before, in case of attributes with multiple possible values (like class), the search will try to match every value separately.\n",
    "    - A Boolean means if the element should contain such attribute or not.\n",
    "    - A custom function that receives the attribute as parameter and returns True or False, depending on whether it matches or not.\n",
    "\n",
    "Aside from the find_* functions, the `select` function can also be used to search based on CSS selectors:\n",
    "```Python\n",
    "soup.select('ul>li.main')   # Select a list of elements based on a selector\n",
    "```\n",
    "\n",
    "Calling a tag without any function is like calling `find_all`:\n",
    "```Python\n",
    "soup('a')   # Find all a elements\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "From https://realpython.github.io/fake-jobs/ find all the job offers for the location state of 'AE' and print the job offer's title, location and apply link. \n",
    "\n",
    "How many offers are with these characteristics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.6 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/local/bin/python3.10 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "\n",
    "# Download the webpage with requests module. Setting timeout just in case\n",
    "res = requests.get('https://realpython.github.io/fake-jobs/', timeout=30)\n",
    "if not res.ok:\n",
    "    print(\"Error: Can't download webpage\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Start the parser (basic parser: 'html.parser')\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "# Find the job offer cards. The ones that have 'card' in the class\n",
    "jobs = soup.find_all(class_='card')\n",
    "\n",
    "# Let's count how many job offers\n",
    "job_count = 0\n",
    "\n",
    "# Traverse all job offers\n",
    "for job in jobs:\n",
    "    # For every job:\n",
    "    # Get the job location. It is the HTML element with class 'location'.\n",
    "    # Get the text node of the element and strip it.\n",
    "    location = job.find(class_='location').text.strip()\n",
    "\n",
    "    # Get the state from the location\n",
    "    state = location.split(',')[1].strip()\n",
    "\n",
    "    # If the state is 'AE', list job title, location and apply link\n",
    "    if state == 'AE':\n",
    "        # Title is the H2 element with class 'title'. Get the text node of the element.\n",
    "        title = job.find('h2', class_='title').text\n",
    "\n",
    "        # The apply link is the link with 'Apply' as text\n",
    "        # Find the text node and navigate to the parent (a element) to get the href attribute\n",
    "        link = job.find(text='Apply').parent['href']\n",
    "        # Print the desired fields of the job offer\n",
    "        print(f'Title: {title}\\nLocation: {location}\\nApply here: {link}\\n\\n')\n",
    "\n",
    "        # Increment job count\n",
    "        job_count += 1\n",
    "\n",
    "# Print the total number of job offers\n",
    "print(f'Job count: {job_count}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "610c699f0cd8c4f129acd9140687fff6866bed0eb8e82f249fc8848b827b628c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
